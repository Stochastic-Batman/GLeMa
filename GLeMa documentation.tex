\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{geometry}
\usepackage{graphicx}


\title{GLeMa: Maximum Likelihood Estimation for Generalized Linear Models}
\author{Stochastic Batman}
\date{December 2025}


\begin{document}
	\maketitle
	
	\section{Maximum Likelihood Estimation}
	\label{sec:mle}
	
	Maximum Likelihood Estimation (MLE) is fundamentally a technique that operates in the reverse direction of traditional probability. While probability functions calculate the likelihood of observing data given specified parameters, MLE seeks to identify the optimal parameters that maximize the likelihood of observing the given data. This method is essential for many statistical procedures and forms the backbone of various algorithms used in data science and machine learning, such as the Naive Bayes classifier \cite{NBC}, Gaussian mixture models \cite{GMM}, and more. By efficiently identifying parameters, MLE enhances model performance and predictive accuracy, allowing for robust analysis and decision-making based on empirical data. This section is based on the works from \cite{psustat}.
	
	\subsection*{Intuitive Explanation}
	
	The basic intuition behind Maximum Likelihood Estimation is grounded in probability. Suppose we have a random sample of data, $X_1, X_2, \dots, X_n$, and we assume this data comes from a specific probability distribution (like a Normal or Bernoulli distribution). This distribution depends on some unknown parameter, denoted by $\theta$.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{images/normal_distributions_with_different_means_and_stds.png}
		\caption{Normal distributions with different means and standard deviations}
		\label{fig:normal_distributions}
	\end{figure}
	
	The method suggests that the optimal parameter value is the specific $\theta$ that maximizes the probability (or likelihood) of having generated the data we actually collected.
	
	In a nutshell: We treat the probability of our data as a function of the unknown parameter. We then ask, ``Which value of the parameter makes our observed data most likely to have happened?''
	
	\subsection*{The Likelihood Function}
	\label{sec:likelihood}
	
	\textbf{Definition:}
	Let $X_1, X_2, \dots, X_n$ be a random sample from a distribution that depends on one or more unknown parameters $\theta_1, \theta_2, \dots, \theta_m$ with probability density (or mass) function $f(x_i; \theta_1, \dots, \theta_m)$. Suppose that the parameter vector $(\theta_1, \dots, \theta_m)$ is restricted to a given parameter space $\Omega$.
	
	The joint probability density (or mass) function of $X_1, X_2, \dots, X_n$, when regarded as a function of $\boldsymbol{\theta} = \theta_1, \theta_2, \dots, \theta_m$, is called the \textbf{Likelihood Function}, denoted by $L$:
	
	$$ L(\boldsymbol{\theta}) = \mathbb{P}(X_1 = x_1, X_2 = x_2, \cdots, X_n = x_n) = \prod_{i=1}^{n} f(x_i; \boldsymbol{\theta}) $$
	
	The first equality is merely the definition of the joint probability mass function. The second equality stems from the fact that we have a random sample, which inherently means that the samples are independent. 
	
	The function is defined for all parameter vectors $(\theta_1, \dots, \theta_m)$ in the parameter space $\Omega$.
	
	\subsubsection*{Example: Likelihood for Bernoulli Data}
	
	Suppose we have a random sample of $n$ independent coin flips $X_1, \dots, X_n$, where $X_i \in \{0, 1\}$, and the probability of success is $p$. The probability mass function is $f(x_i; p) = p^{x_i} (1-p)^{1-x_i}$. The likelihood function $L(p)$ is the product of the individual probabilities:
	$$ L(p) = \prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} = p^{\sum x_i} (1-p)^{n - \sum x_i} $$
	
	\subsubsection*{The Necessity of Log-Likelihood}
	
	For large sample sizes, the Likelihood Function $L(\theta)$, which is a product of many probability values (each between 0 and 1), quickly becomes an extremely small number. This causes two main problems:
	\begin{enumerate}
		\item \textbf{Numerical Underflow:} Computational systems may fail to store these tiny numbers accurately, leading to a value of zero (underflow).
		\item \textbf{Calculus Complexity:} Products are difficult to differentiate. By taking the logarithm, the product simplifies to a sum, which is mathematically much easier to optimize.
	\end{enumerate}
	
	\subsubsection*{Mathematical Justification for Log-Likelihood}
	
	The use of the natural logarithm is justified because it is a \textit{strictly monotonically increasing function}. This property ensures that the maximum point of the Likelihood function $L(\theta)$ is exactly the same as the maximum point of the Log-Likelihood function $\ln L(\theta)$.
	
	If we consider any two parameter values $\theta_a$ and $\theta_b$:
	$$ L(\theta_a) > L(\theta_b) \iff \ln(L(\theta_a)) > \ln(L(\theta_b)) $$
	
	Therefore, maximizing the simpler function, $\ln L(\theta)$, yields the identical maximum likelihood solution, $\hat{\theta}$. The optimization procedure involves differentiating the Log-Likelihood, setting it to zero, and solving for $\theta$:
	$$ \frac{\partial \ln L(\theta)}{\partial \theta} \overset{\text{set}}{=} 0 $$
	
	\subsubsection*{Example: Log-Likelihood for Bernoulli Data}
	
	Applying the natural logarithm to the Bernoulli likelihood function $L(p)$ from the example above:
	
	$$ \ln L(p) = \ln \left( p^{\sum x_i} (1-p)^{n - \sum x_i} \right) $$
	Using the logarithm property $\ln(a^b c^d) = b\ln(a) + d\ln(c)$:
	$$ \ln L(p) = \left(\sum_{i=1}^{n} x_i\right) \ln(p) + \left(n - \sum_{i=1}^{n} x_i\right) \ln(1-p) $$
	
	Therefore, we have (remember, the necessary condition for a maximum is that the derivative equals $0$):
	
	$$ \frac{\partial \ln L(p)}{\partial p} = \frac{\sum x_i}{p} - \frac{n - \sum x_i}{1 - p} \text{ which we set to } 0$$
	
	Then: $$ \left( \sum x_i \right) (1 - p) - \left( n - \sum x_i \right) p = 0 \implies \sum x_i - np = 0$$
	
	Solving for $p$ (hat on the parameter $\hat{p}$ indicates that it is an estimate) to get an estimate:
	
	$$ \hat{p} = \frac{\sum_{i = 1}^n x_i}{n} \text{ or, alternatively, an estimator: } \hat{p} = \frac{\sum_{i = 1}^n X_i}{n} $$
	
	\subsection*{Estimators vs. Estimates}
	
	A common source of confusion in mathematical statistics is the distinction between an \textit{estimator} and an \textit{estimate}. While used interchangeably in casual conversation, they represent distinct mathematical concepts.
	
	\subsubsection*{The Recipe vs. The Meal}
	
	Before defining formally, to understand the difference intuitively, consider the analogy of cooking:
	
	\begin{enumerate}
		\item \textbf{The Estimator (The Recipe):} Think of an estimator as a \textit{rule} or a \textit{formula}. It tells you how to process ingredients (data) to get a result. Before you actually cook, the recipe exists as a procedure. In statistics, the estimator is the formula we choose to use (e.g., "I will calculate the average of whatever data I collect"). Because we haven't collected the data yet, the result of the estimator is unknown and random.
		
		\item \textbf{The Estimate (The Meal):} Think of the estimate as the specific result you get after you have finished cooking. It is a concrete, fixed product. In statistics, once you collect your specific data points and plug them into the formula, you get a single number (e.g., "The average is 5.2"). This number is the estimate.
	\end{enumerate}
	
	\subsubsection*{The Statistic}
	A \textbf{statistic} is formally defined as any function of the observable random variables in a sample that does not depend on any unknown parameters. \\
	
	Let $X_1, X_2, \dots, X_n$ be a random sample. A statistic $T$ is any function: $$ T = g(X_1, X_2, \dots, X_n) $$
	
	Since $T$ is a function of random variables, the statistic $T$ itself is a random variable and has a probability distribution (called its sampling distribution).
	
	\subsubsection*{The Estimator}
	An \textbf{estimator} is a statistic, which is a function of the random sample $X_1, X_2, \dots, X_n$. Because the sample variables $X_i$ are random variables (before the experiment is performed), the estimator is also a random variable. \\
	
	Let $\Theta$ be the parameter of interest. An estimator $\hat{\Theta}$ is defined as:	$$ \hat{\Theta} = h(X_1, X_2, \dots, X_n) $$
	
	The function $h$ is the mathematical \textbf{rule or procedure} that maps the values of the random sample space (the observed data) to a value in the parameter space (the estimate). It represents the specific formula chosen for estimation (e.g., the formula for the sample mean or the sample variance). The choice of $h$ determines the properties of the estimator $\hat{\Theta}$. \\
	
	\subsubsection*{The Estimate}
	An \textbf{estimate} is a specific realization of the estimator. It is calculated using the observed values $x_1, x_2, \dots, x_n$ (after the experiment is performed).
	
	Let $x_1, \dots, x_n$ be the observed data. The estimate $\hat{\theta}$ is: $$ \hat{\theta} = h(x_1, x_2, \dots, x_n) $$
	
	\begin{table}[H]
		\centering
		\begin{tabular}{@{}lll@{}}
			\toprule
			\textbf{Feature} & \textbf{Estimator} & \textbf{Estimate} \\ \midrule
			\textbf{Symbol} & $\hat{\Theta}$ or $\bar{X}$ (Capital) & $\hat{\theta}$ or $\bar{x}$ (Lowercase) \\
			\textbf{Mathematical Nature} & Random Variable (Function) & Constant (Real Number) \\
			\textbf{Input} & Random Sample $X_1, \dots, X_n$ & Observed Data $x_1, \dots, x_n$ \\
			\textbf{Timing} & Pre-data collection & Post-data collection \\
			\textbf{Properties} & Bias, Variance, Consistency & Accuracy (for a specific case) \\ \bottomrule
		\end{tabular}
		\caption{Comparison of Estimators and Estimates}
		\label{tab:estimator_vs_estimate}
	\end{table}
	
	\section{Generalized Linear Models}
	\label{sec:glm}
	
	A generalized linear model (GLM) is a statistical framework for analyzing relationships between variables where the outcomes, referred to as response variables, can follow various patterns - not just a straight-line relationship. The response variable is the specific outcome being predicted or explained, such as the number of successes in a series of trials or the probability of an event occurring. GLMs enable the linking of these response variables to one or more influencing factors, accommodating scenarios where the outcomes are counts, proportions, or other types of data rather than just averages.
	
	Understanding traditional linear models is essential before exploring the complexities of GLMs. Traditional linear models establish a direct relationship between a response variable and one or more predictor variables, providing the foundation for the more flexible approach offered by GLMs.
	
	\subsection*{Linear Models}
	
	The simplest and most intuitive way to model the relationship between variables is to assume a straight-line connection. Imagine trying to predict someone's weight based on their height - we might reasonably expect that taller people tend to weigh more, and this relationship could be approximated by a line. Linear models capture this idea mathematically, providing a framework for understanding how changes in one variable correspond to changes in another.
	
	In a linear model, we express the response variable as a weighted combination of predictor variables plus some random noise that accounts for natural variability and measurement error. The goal is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the discrepancies between our predictions and the actual observed values.
	
	\subsubsection*{The Linear Model Framework}
	
	Formally, a linear model expresses the relationship between a response variable $Y$ and predictor variables $X_1, X_2, \dots, X_p$ as:
	
	$$ Y_i = \beta_0 + \left( \sum \limits_{j = 1}^p \beta_j X_{i_j} \right) + \epsilon_i $$
	
	where:
	\begin{itemize}
		\item $Y_i$ is the $i$-th observation of the response variable
		\item $X_{i_j}$ represents the $j$-th predictor variable for the $i$-th observation
		\item $\beta_0$ is the intercept term (the expected value of $Y$ when all predictors are zero)
		\item $\beta_1, \beta_2, \dots, \beta_p$ are the regression coefficients (slopes) that quantify the effect of each predictor
		\item $\epsilon_i$ is the random error term for observation $i$ assumed to be independent and identically distributed (i.i.d.) as $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$
	\end{itemize}
	
	In matrix notation, the linear model for all $n$ observations can be written compactly as (with the first column of $\mathbf{X}$ typically being all ones for the intercept):
	
	$$ \mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}, \text{ with } \mathbf{Y} \in \mathbb{R}^{n}, \, \mathbf{X} \in \mathbb{R}^{n \times (p + 1)}, \, \boldsymbol{\beta} \in \mathbb{R}^{(p + 1) \times 1}, \, \boldsymbol{\epsilon} \in \mathbb{R}^{n} $$
	
	\subsubsection*{Maximum Likelihood Estimation for Linear Models}
	
	Under the normality assumption for the errors, the response variable $Y_i$ follows a normal distribution:
	
	$$ Y_i \sim \mathcal{N}(\mu_i, \sigma^2) \quad \text{where} \quad \mu_i = \beta_0 + \left( \sum \limits_{j = 1}^p \beta_j X_{i_j} \right) $$
	
	The probability density function for observation $Y_i$ is:
	
	$$ f(y_i; \boldsymbol{\beta}, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right) $$
	
	Following the procedure outlined in Section \ref{sec:mle}, the likelihood function for all $n$ independent observations is:
	
	$$ L(\boldsymbol{\beta}, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right) $$
	
	Taking the natural logarithm yields the log-likelihood function:
	
	$$ \begin{aligned}
	\ln L(\boldsymbol{\beta}, \sigma^2) &= \ln \left[ \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right) \right] \\
		&= \sum_{i=1}^{n} \ln \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right) \right] \\
		&= \sum_{i=1}^{n} \left[ \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \ln\left(\exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)\right) \right] \\
		&= \sum_{i=1}^{n} \left[ \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) - \frac{(y_i - \mu_i)^2}{2\sigma^2} \right] \\
		&= \sum_{i=1}^{n} \left[ -\frac{1}{2}\ln(2\pi\sigma^2) - \frac{(y_i - \mu_i)^2}{2\sigma^2} \right] \\
		&= \sum_{i=1}^{n} \left[ -\frac{1}{2}\ln(2\pi) - \frac{1}{2}\ln(\sigma^2) - \frac{(y_i - \mu_i)^2}{2\sigma^2} \right] \\
		&= -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mu_i)^2
	\end{aligned} $$
	
	Substituting $\mu_i = \mathbf{x}_i^T\boldsymbol{\beta}$ (where $\mathbf{x}_i$ is the $i$-th row of $\mathbf{X}$ as a column vector):
	
	$$ \ln L(\boldsymbol{\beta}, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 $$
	
	To find the maximum likelihood estimator $\hat{\boldsymbol{\beta}}$, we differentiate with respect to $\boldsymbol{\beta}$ and set equal to zero:
	
	$$ \frac{\partial \ln L(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}} = \frac{1}{\sigma^2}\sum_{i=1}^{n}\mathbf{x}_i(y_i - \mathbf{x}_i^T\boldsymbol{\beta}) = \mathbf{X}^T(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}) \overset{\text{set}}{=} \mathbf{0} $$
	
	Solving this equation yields the "normal equations":
	
	$$ \mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^T\mathbf{Y} $$
	
	Assuming $\mathbf{X}^T\mathbf{X}$ is invertible, the maximum likelihood estimator for $\boldsymbol{\beta}$ is:
	
	$$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} $$
	
	This is the ordinary least squares (OLS) estimator. Remarkably, under the normality of the error term assumption, the maximum likelihood estimator and the least squares estimator coincide.
	
	For the variance parameter, differentiating the log-likelihood with respect to $\sigma^2$ and setting to zero yields:
	
	$$ \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(y_i - \mathbf{x}_i^T\hat{\boldsymbol{\beta}})^2 = \frac{1}{n}(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) $$
	
	\subsubsection*{Limitations of Linear Models}
	
	While linear models are powerful and widely applicable, they impose several restrictive assumptions that may not hold in many practical scenarios:
	
	\begin{enumerate}
		\item \textbf{Normality of Errors:} The assumption that $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ may be violated when the response variable is binary, count-based, or highly skewed.
		
		\item \textbf{Constant Variance (Homoscedasticity):} Linear models assume that the variance of the error terms is constant across all levels of the predictors. In reality, variance often depends on the mean (e.g., count data typically exhibit variance proportional to the mean).
		
		\item \textbf{Unbounded Predictions:} The linear predictor $\mathbf{X}\boldsymbol{\beta}$ can take any real value from $-\infty$ to $+\infty$. This is problematic when modeling probabilities (which must lie in $[0,1]$), counts (which must be non-negative), or other constrained quantities.
		
		\item \textbf{Linear Relationship:} The model assumes that the expected value of $Y$ is a linear function of the predictors. Non-linear relationships cannot be captured without transformation.
	\end{enumerate}
	
	These limitations motivate the development of Generalized Linear Models, which relax many of these assumptions while retaining the interpretability and computational advantages of the linear framework. GLMs achieve this flexibility by introducing a link function that connects the linear predictor to the expected value of the response through a non-linear transformation, and by allowing the response to follow distributions beyond the normal distribution - including binomial, Poisson, gamma, and others from the exponential family.
	
	\subsection*{Structure of Generalized Linear Models}
	
	Generalized Linear Models extend the linear modeling framework by relaxing the restrictive assumptions imposed by ordinary linear models. While linear models are limited to normally distributed responses with constant variance, GLMs accommodate a much broader class of response distributions and allow the variance to depend on the mean in a natural way.
	
	\subsubsection*{Exponential Family}
	
	Most commonly used statistical distributions belong to the exponential family, whose probability density (or mass) functions can be written in the canonical form:
	$$ f(y; \theta, \phi) = \exp \left( \frac{y\theta - b(\theta)}{\phi} + c(y, \phi) \right) $$
	
	where:
	\begin{itemize}
		\item $\theta$ is the canonical parameter (also called the natural parameter)
		\item $\phi$ is the dispersion parameter
		\item $b(\theta)$ and $c(y, \phi)$ are specific functions that define the distribution
	\end{itemize}
	
	A remarkable property of the exponential family is that the mean and variance can be derived directly from the function $b(\theta)$:
	$$ \mathbb{E}[Y] = b'(\theta) = \mu $$
	$$ \text{var}(Y) = \phi b''(\theta) = \phi V(\mu) $$
	
	where $b'(\theta)$ and $b''(\theta)$ denote the first and second derivatives of $b$ with respect to $\theta$.
	
	\subsubsection*{The Three Components of a GLM}
	
	A GLM consists of three components that work together to model the relationship between predictors and response: 
	
	\begin{enumerate}
		\item \textbf{The Random Component:} This specifies the probability distribution of the response variable $Y_i$. In a GLM, we assume that $Y_i$ follows a distribution from the \textit{exponential family}, which includes:
		\begin{itemize}
			\item Normal distribution (for continuous data)
			\item Binomial distribution (for binary or proportion data)
			\item Poisson distribution (for count data)
			\item Gamma distribution (for positive continuous data)
		\end{itemize}
	
	\item \textbf{The Systematic Component:} This is the linear predictor, which combines the predictor variables linearly:
	$$ \eta_i = \beta_0 + \beta_1 x_{i_1} + \beta_2 x_{i_2} + \cdots + \beta_p x_{i_p} = \sum_{j=0}^{p} \beta_j x_{ij} $$
	
	where $x_{i_0} = 1$ for all $i$ to account for the intercept term. The linear predictor $\eta_i$ can take any real value in $\mathbb{R}$.
	
	\item \textbf{The Link Function:} This function $g(\cdot)$ connects the mean of the response variable $\mu_i = \mathbb{E}(Y_i)$ to the linear predictor:
	$$ g(\mu_i) = \eta_i $$
	
	The link function transforms the expected value of the response (which may be constrained to a specific range) to the scale of the linear predictor (which is unrestricted). The inverse link function is denoted as $g^{-1}$, so that:
	$$ \mu_i = g^{-1}(\eta_i) $$
	\end{enumerate}
	
	\subsubsection*{The Variance Function}
	
	In addition to these three components, GLMs specify how the variance of the response depends on its mean through a variance function:
	$$ \text{var}(Y_i) = \phi V(\mu_i) $$
	
	where $\phi$ is the dispersion parameter (a constant) and $V(\mu_i)$ is the variance function that describes how variance changes with the mean. For the normal distribution, $V(\mu_i) = 1$ (constant variance), but for other distributions, the variance typically depends on $\mu_i$.
	
	\subsubsection*{Canonical Links}
	
	For any exponential family distribution, there exists a special link function called the \textbf{canonical link}. The canonical link is defined as:
	$$ g(\mu_i) = \theta_i $$
	
	In other words, the canonical link function sets the linear predictor equal to the canonical parameter. This gives:
	$$ \theta_i = \eta_i = \beta_0 + \beta_1 x_{i_1} + \cdots + \beta_p x_{i_p} $$
	
	Canonical links lead to simpler mathematical forms for the likelihood equations and maximum likelihood estimation becomes numerically more stable. However, the canonical link is not always the most appropriate choice for modeling. While it offers computational and theoretical advantages, the choice of link function should ultimately be guided by the scientific context and the interpretability of the model parameters.
	
	\subsection*{Common GLM Families and Their Properties}
	
	Every probability and statistics course covers these distributions, but one might not be very familiar with the distributions in the exponential family form.
	
	\subsubsection*{Normal Distribution (Gaussian Family)}
	
	For the normal distribution with mean $\mu$ and variance $\sigma^2$:
	
	\textbf{Canonical form:}
	$$ f(y; \mu, \sigma^2) = \exp\left( \frac{y\mu - \frac{1}{2}\mu^2}{\sigma^2} - \frac{1}{2}\left(\frac{y^2}{\sigma^2} + \log(2\pi\sigma^2)\right) \right) $$
	
	\vspace*{20px}
	\textbf{Properties:}
	\begin{itemize}
		\item Canonical parameter: $\theta = \mu$
		\item $b(\theta) = \frac{1}{2}\theta^2$
		\item Dispersion parameter: $\phi = \sigma^2$
		\item Variance function: $V(\mu) = 1$
		\item Canonical link: $g(\mu) = \mu$ (identity link)
		\item Mean-variance relationship: $\text{var}(Y) = \sigma^2$ (constant)
	\end{itemize}
	
	This is precisely the ordinary linear model, showing that linear regression is a special case of GLMs.
	
	\subsubsection*{Binomial Distribution from \cite{GLM}}
	
	Suppose $Y_i \sim \text{Binomial}(n_i, p_i)$, where $n_i$ is the number of trials and $p_i$ is the probability of success. The proportions $Y_i/n_i$ are typically modeled. \\
	
	\textbf{Properties:}
	\begin{itemize}
		\item Mean: $\mathbb{E}(Y_i/n_i) = p_i$
		\item Variance: $\displaystyle \text{var} \left( Y_i / n_i \right) = \frac{1}{n_i}p_i(1-p_i)$
		\item Canonical parameter: $\theta = \log\left(\frac{p}{1-p}\right)$ (log-odds)
		\item Dispersion parameter: $\phi = 1$ (known)
		\item Variance function: $V(\mu_i) = \mu_i(1-\mu_i)$
		\item Canonical link: $g(\mu_i) = \log\left(\frac{\mu_i}{1-\mu_i}\right)$ (logit link)
	\end{itemize}
	
	The logit link function maps probabilities from the interval $(0,1)$ to the entire real line $(-\infty, \infty)$, making it appropriate for the linear predictor. The inverse logit (logistic function) is:
	$$ \mu_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} = \frac{1}{1 + \exp(-\eta_i)} $$
	
	Alternative link functions for binomial data include:
	\begin{itemize}
		\item Probit link: $g(\mu) = \Phi^{-1}(\mu)$ where $\Phi$ is the standard normal CDF
		\item Complementary log-log link: $g(\mu) = \log(-\log(1-\mu))$
	\end{itemize}
	
	\subsubsection*{Poisson Distribution}
	
	Suppose $Y_i \sim \text{Poisson}(\lambda_i)$, typically used for modeling count data. \\
	
	\textbf{Properties:}
	\begin{itemize}
		\item Mean: $\mathbb{E}(Y_i) = \lambda_i$
		\item Variance: $\text{var}(Y_i) = \lambda_i$
		\item Canonical parameter: $\theta = \log(\lambda)$
		\item Dispersion parameter: $\phi = 1$ (known)
		\item Variance function: $V(\mu_i) = \mu_i$
		\item Canonical link: $g(\mu_i) = \log(\mu_i)$ (log link)
	\end{itemize}
	
	The log link ensures that the predicted mean is always positive, which is necessary for count data. The inverse link is:
	$$ \mu_i = \exp(\eta_i) $$
	
	This means that the effects of predictors are multiplicative on the original scale:
	$$ \mathbb{E}[Y_i] = \exp(\beta_0 + \beta_1 x_{i_1} + \cdots + \beta_p x_{i_p}) $$
	
	A one-unit increase in predictor $x_j$ multiplies the expected count by a factor of $\exp(\beta_j)$.
	
	\subsection*{Maximum Likelihood Estimation for GLMs}
	
	Parameter estimation in GLMs is performed using maximum likelihood. For a sample $y_1, y_2, \dots, y_n$ from an exponential family distribution, the log-likelihood is:
	$$ L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left( \frac{y_i\theta_i - b(\theta_i)}{\phi_i} + c(y_i, \phi_i) \right) $$
	
	From \cite{GLM}, the maximum likelihood estimates are obtained by solving:
	$$ \frac{\partial L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} \frac{y_i - \mu_i}{\phi_i V(\mu_i)} \times \frac{x_{ij}}{g'(\mu_i)} = 0 $$
	
	for each parameter $\beta_j$, where $g'(\mu_i)$ is the derivative of the link function.
	
	\subsubsection*{Iteratively Reweighted Least Squares (IRLS)}
	
	Unlike ordinary linear models where a closed-form solution exists, GLMs generally require iterative numerical methods. The standard algorithm is Iteratively Reweighted Least Squares (IRLS), also known as Fisher's Method of Scoring.
	
	\begin{algorithm}
		\caption{Iteratively Reweighted Least Squares (IRLS) for GLMs}
		\begin{algorithmic}[1]
			\Require Design matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$, response vector $\mathbf{y} \in \mathbb{R}^n$, link function $g(\cdot)$, variance function $V(\cdot)$, prior weights $\mathbf{a} \in \mathbb{R}^n$, tolerance $\epsilon > 0$, maximum iterations $M$
			\Ensure Parameter estimates $\hat{\boldsymbol{\beta}} \in \mathbb{R}^p$
			
			\State \textbf{Initialize:} Set $\boldsymbol{\beta}^{(0)} \gets \mathbf{0}$ (or other suitable starting values)
			\State Compute $\boldsymbol{\eta}^{(0)} \gets \mathbf{X}\boldsymbol{\beta}^{(0)}$
			\State Compute $\boldsymbol{\mu}^{(0)} \gets g^{-1}(\boldsymbol{\eta}^{(0)})$ \Comment{Apply inverse link elementwise}
			\State Set $r \gets 0$
			
			\Repeat
			\State \textbf{// Step 2: Compute working responses}
			\For{$i = 1$ to $n$}
			\State $z_i^{(r)} \gets \eta_i^{(r)} + (y_i - \mu_i^{(r)}) \cdot g'(\mu_i^{(r)})$
			\EndFor
			
			\State \textbf{// Step 3: Compute iterative weights}
			\For{$i = 1$ to $n$}
			\State $w_i^{(r)} \gets \dfrac{a_i}{V(\mu_i^{(r)}) \cdot [g'(\mu_i^{(r)})]^2}$
			\EndFor
			\State $\mathbf{W}^{(r)} \gets \text{diag}(w_1^{(r)}, w_2^{(r)}, \ldots, w_n^{(r)})$
			
			\State \textbf{// Step 4: Weighted least squares update}
			\State $\boldsymbol{\beta}^{(r+1)} \gets (\mathbf{X}^T \mathbf{W}^{(r)} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W}^{(r)} \mathbf{z}^{(r)}$
			
			\State \textbf{// Step 5: Update fitted values}
			\State $\boldsymbol{\eta}^{(r+1)} \gets \mathbf{X}\boldsymbol{\beta}^{(r+1)}$
			\State $\boldsymbol{\mu}^{(r+1)} \gets g^{-1}(\boldsymbol{\eta}^{(r+1)})$
			
			\State \textbf{// Check convergence}
			\State $\delta \gets \|\boldsymbol{\beta}^{(r+1)} - \boldsymbol{\beta}^{(r)}\|_2$
			\State $r \gets r + 1$
			
			\Until{$\delta < \epsilon$ \textbf{or} $r \geq M$}
			
			\State \Return $\boldsymbol{\beta}^{(r)}$
		\end{algorithmic}
	\end{algorithm}
	
	For models using the canonical link, IRLS simplifies to the Newton-Raphson method, as the expected and observed information matrices coincide.
	
	\subsubsection*{Asymptotic Properties and Inference}
	
	The maximum likelihood estimator $\hat{\boldsymbol{\beta}}$ has desirable asymptotic properties. As the sample size $n \to \infty$:
	$$ \hat{\boldsymbol{\beta}} \xrightarrow{d} \mathcal{N}(\boldsymbol{\beta}, \mathbf{I}^{-1}(\boldsymbol{\beta})) $$
	
	where $\mathbf{I}(\boldsymbol{\beta})$ is the Fisher information matrix:
	$$ \mathbf{I}(\boldsymbol{\beta}) = \phi^{-1} \mathbf{X}^T \mathbf{W} \mathbf{X} $$
	
	The estimated covariance matrix of $\hat{\boldsymbol{\beta}}$ is:
	$$ \widehat{\text{cov}}(\hat{\boldsymbol{\beta}}) = \hat{\phi}(\mathbf{X}^T \hat{\mathbf{W}} \mathbf{X})^{-1} $$
	
	where $\hat{\mathbf{W}}$ is computed at the final iteration of IRLS.
	
	Standard errors for individual parameters are obtained as:
	$$ \text{SE}(\hat{\beta}_j) = \sqrt{[\widehat{\text{cov}}(\hat{\boldsymbol{\beta}})]_{jj}} $$
	
	For testing individual coefficients, we can use Wald tests. To test $H_0: \beta_j = 0$ versus $H_1: \beta_j \neq 0$:
	$$ z_j = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} $$
	
	which is asymptotically $\mathcal{N}(0,1)$ under $H_0$ when $\phi$ is known (e.g., for binomial and Poisson models). When $\phi$ must be estimated (e.g., for normal and gamma models), a $t$-distribution with $n-p$ degrees of freedom is used instead.
	
	\subsection*{Deviance and Model Comparison}
	
	The deviance is a key measure of model fit for GLMs, analogous to the residual sum of squares in linear models. It is defined as:
	$$ D = 2\phi(L_{\text{sat}} - L_{\text{mod}}) $$
	
	where $L_{\text{mod}}$ is the log-likelihood of the fitted model and $L_{\text{sat}}$ is the log-likelihood of the saturated model.
	
	The saturated model is one that fits the data perfectly by having as many parameters as observations, so that $\hat{y}_i = y_i$ for all $i$. The deviance measures how far the fitted model deviates from perfect fit.
	
	For models with dispersion parameter $\phi = 1$ (binomial and Poisson), the deviance is:
	$$ D = 2\sum_{i=1}^{n} \left[ y_i(\tilde{\theta}_i - \hat{\theta}_i) - b(\tilde{\theta}_i) + b(\hat{\theta}_i) \right] $$
	
	where $\tilde{\theta}_i$ corresponds to the saturated model and $\hat{\theta}_i$ corresponds to the fitted model.
	
	\subsubsection*{Specific Deviance Forms}
	
	\textbf{Normal (Gaussian):}
	$$ D = \sum_{i=1}^{n} (y_i - \hat{\mu}_i)^2 $$
	This is exactly the residual sum of squares.
	
	\textbf{Binomial:}
	$$ D = 2\sum_{i=1}^{n} \left[ y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) + (n_i - y_i)\log\left(\frac{n_i - y_i}{n_i - \hat{\mu}_i}\right) \right] $$
	
	\textbf{Poisson:}
	$$ D = 2\sum_{i=1}^{n} \left[ y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i) \right] $$
	
	\subsubsection*{Comparing Nested Models}
	
	For nested models (where one model is a special case of another), we can test whether the additional parameters significantly improve the fit using a likelihood ratio test.
	
	Let $M_0$ be the simpler model (with $p_0$ parameters) and $M_1$ be the more complex model (with $p_1 > p_0$ parameters). The test statistic is:
	$$ \Delta D = D_0 - D_1 $$
	
	Under $H_0$ (that the simpler model is adequate), for models with $\phi = 1$:
	$$ \Delta D \xrightarrow{d} \chi^2_{p_1 - p_0} $$
	
	For models where $\phi$ must be estimated, we use an $F$-test:
	$$ F = \frac{(D_0 - D_1)/(p_1 - p_0)}{\hat{\phi}} \sim F_{p_1-p_0, n-p_1} $$
	
	\subsection*{Model Selection Criteria}
	
	\subsubsection*{Akaike Information Criterion (AIC)}
	
	The AIC provides a measure of model quality that balances goodness of fit with model complexity:
	$$ \text{AIC} = -2L_{\text{mod}} + 2p $$
	
	where $p$ is the number of parameters in the model. Lower AIC values indicate better models. The AIC penalizes overly complex models, helping to avoid overfitting.
	
	When comparing multiple models, we select the one with the minimum AIC. The AIC can be used to compare non-nested models, unlike the likelihood ratio test.
	
	\subsubsection*{Bayesian Information Criterion (BIC)}
	
	An alternative to AIC is the BIC (also called Schwarz criterion):
	$$ \text{BIC} = -2L_{\text{mod}} + p\log(n) $$
	
	The BIC imposes a stronger penalty for model complexity than the AIC, especially for large sample sizes. This tends to favor more parsimonious models.
	
	\subsection*{Residuals for GLMs}
	
	Several types of residuals can be defined for GLMs, each serving different diagnostic purposes:
	
	\textbf{Response Residuals:}
	$$ r_i = y_i - \hat{\mu}_i $$
	
	\textbf{Pearson Residuals:}
	$$ r_i^P = \frac{y_i - \hat{\mu}_i}{\sqrt{V(\hat{\mu}_i)}} $$
	
	These are standardized by the estimated standard deviation of $Y_i$.
	
	\textbf{Deviance Residuals:}
	$$ r_i^D = \text{sign}(y_i - \hat{\mu}_i) \sqrt{d_i} $$
	
	where $d_i$ is the contribution of observation $i$ to the total deviance, and $\text{sign}(\cdot)$ ensures the residual has the same sign as $y_i - \hat{\mu}_i$. The sum of squared deviance residuals equals the deviance: $\sum_{i=1}^{n} (r_i^D)^2 = D$.
	
	Deviance residuals are the default in most statistical software for GLM diagnostics because they directly relate to the fitting criterion used in maximum likelihood estimation.

	
	\newpage
	\begin{thebibliography}{1}	
		\bibitem{NBC}
		Langley, P., \& Sage, S. (1994). \textbf{The importance of naive Bayes}. In \textit{Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence} (pp. 333-337). Morgan Kaufmann.
		
		\bibitem{GMM}
		McLachlan, G. J., \& Basford, K. E. (1988). \textbf{Mixture Models: Inference and Applications to Clustering}. Wiley.
		
		\bibitem{psustat}
		\textbf{PennState - Eberly College of Science.} STAT 415 - Introduction to Mathematical Statistics, 1.2 - Maximum Likelihood Estimation. From \texttt{https://online.stat.psu.edu/stat415/lesson/1/1.2} 
		
		\bibitem{GLM}
		Turner, H. \textbf{Introduction to Generalized Linear Models}. ESRC National Centre for Research Methods, UK, and Department of Statistics, University of Warwick, UK. Retrieved from \texttt{https://statmath.wu.ac.at/courses/heather\_turner/glmCourse\_001.pdf}
		
		\bibitem{glema}
		\textbf{GLeMa GitHub Repository.} MLE meets GLM. \texttt{https://github.com/Stochastic-Batman/GLeMa}
		
	\end{thebibliography}
\end{document}